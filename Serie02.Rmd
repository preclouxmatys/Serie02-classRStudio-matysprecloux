title: "Series-02-Rstudio : corelations and statistical test"
author: "Matys PRÉCLOUX"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: false
    number_sections: true
---

# Correlations and linear regression
In this series of exercises, you will explore correlations and linear regression, using a dataset from a real experiment.

## The problem
You want to know if there is a relationship between the nonuse of the proximal part of the upper limb (PANU) and the nonuse of the shoulder (SANU) or the elbow (EENU).

## The data
As you will see, the data is far from perfect, but this is the reality of experimental data, especially in the case of clinical data.

Download the file NonUse.csv from the course Moodle.

```{r load data}
nonuse <- read.csv("data/test_data/NonUse.csv", sep=',', header=TRUE)
summary(nonuse)

```


This file contains measures of upper extremity nonuse:

    PANU: Proximal Arm Non Use
    SANU: Shoulder Antepulsion Non Use
    EENU: Elbow Extension Non Use

In your notebook, make a clear description of the experiment, the variables and the data. Do not forget to give the units of each variable, and the range of possible values.

| name | description | unit | range |
|--    |---------     |--   |---    |
| PANU | Proximal Arm Non Use         | ‰ | 0-100 |
| SANU | Shoulder Antepulsion Non Use | ‰ | 0-100 |
| EENU | Elbow Extension Non Use      | ‰ | 0-100 |



## Corelation analysis
Below are some questions to guide your analysis.

#### Is PANU correlated with SANU and/or EENU?

we use spearman correlation bc we do not have assumptions about the distributions of the data, and we do not assume linear relationships between the variables 

```{r correlation}
cor.test(nonuse$PANU, nonuse$SANU, method = "spearman")

```
#### with EENU

```{r}
cor.test(nonuse$PANU, nonuse$EENU, method = "spearman")
```


#### Visualize the PANU-SANU relationship and the PANU-EENU relationship

```{r plot sanu+lm}
library(ggplot2)
ggplot(nonuse, aes(x=PANU, y=SANU)) +
  geom_point() +
  geom_smooth(method='lm', formula= y~x, color='blue', se=T) + 
  #xlim(-100, 100) +
  #ylim(-100, 100) +
  labs(title= "PANU vs SANU", x="PANU(x)", y="SANU(%)") + 
  theme_minimal()

library(ggplot2)
ggplot(nonuse, aes(x=PANU, y=EENU)) +
  geom_point() +
  geom_smooth(method='lm', formula= y~x, color='blue', se=T) + 
  #xlim(-100, 100) +
  #ylim(-100, 100) +
  labs(title= "PANU vs EENU", x="PANU(x)", y="EENU(%)") + 
  theme_minimal()

```


#### linear regression for PANU-SANU 

```{r}
lm_sanu <- lm(SANU ~ PANU, data=nonuse)
summary(lm_sanu)
coef(lm_sanu)
slope <- coef(lm_sanu)[2]
intercept <- coef(lm_sanu)[1]
cat(" Regression equation : SANU=", slope, "*PANU +", intercept, "\n")
```
#### linear regression for PANU-EENU

```{r PANU-EENU}
lm_eenu <- lm(EENU ~ PANU, data=nonuse)
summary(lm_eenu)
coef(lm_eenu)
slope <- coef(lm_eenu)[2]
intercept <- coef(lm_eenu)[1]
cat(" Regression equation : EENU=", slope, "*PANU +", intercept, "\n")
```


#### regression equations: PANU = a * SANU + b and PANU = a * EENU + b

```{r equations}
slope <- 0.08932839
intercept <- 0.5860521

a <- 1 / slope
b <- -intercept / slope

cat("Regression equation : PANU =", a, "* SANU +", b, "\n")

slope <- 0.5646221
intercept <- 3.283697
# PANU = a*EENU + b
a <- 1 / slope
b <- -intercept / slope

cat("Regression equation : PANU =", a, "* EENU +", b, "\n")
```


#### Summarize results : 

Our correlation and regression analyses showed that PANU was more strongly associated with EENU than with SANU. The regression equation indicated that a one-unit increase in EENU was associated with an estimated 1.77-unit increase in PANU (PANU = 1.77·EENU – 5.82). By contrast, the relationship with SANU was weaker, with the regression equation suggesting that a one-unit increase in SANU corresponded to an estimated 11.20-unit increase in PANU, but with a higher intercept and greater variability (PANU = 11.20·SANU – 6.56). Taken together, these results support the hypothesis that proximal arm non-use is driven mainly by elbow extension non-use, while shoulder antepulsion non-use plays a less consistent role.

#### What are the limits of your analysis (by ChatGPT)

- The sample size was limited, which reduces statistical power and may exaggerate or obscure true relationships.
- Correlation and linear regression do not establish causality; the observed associations may be influenced by other compensatory strategies or biomechanical factors.
- The analysis was based on a specific reaching task under controlled conditions, which may not generalize to all functional activities.
- Measurement errors (marker placement, motion capture noise, anthropometric estimations) may affect the precision of non-use scores.
- Inter-individual variability (stroke severity, recovery stage, motor strategy) was not controlled, which could contribute to unexplained variance.

# 2.1 Statistical Concepts

### 2. What is a variance? A standard deviation?
- **Variance:** the average of the squared differences between each value and the mean.  
- **Standard deviation (SD):** the square root of the variance; expressed in the same units as the data.  
*(Reference: Altman, 1991; Field, 2017)*

### 3. What is a normal distribution?
A bell-shaped, symmetrical distribution where mean = median = mode.  
About 68% of data lie within 1 SD of the mean, 95% within 2 SD.  
*(Reference: Altman & Bland, 1995, BMJ)*

### 4. What is a statistical test?
A statistical test is a method used to decide, based on data, whether an observed effect is likely real or due to chance.  
*(Reference: Howell, 2013)*

### 5. When to use a statistical test?
When comparing groups, testing relationships, or evaluating hypotheses.  
Examples: comparing treatment vs control, testing correlation between two variables.  
*(Reference: Derrick et al., 2017 )*

### 6. What is a parametric test? A non-parametric test?
- **Parametric tests:** assume that data follow a certain distribution (usually normal). Examples: t-test, ANOVA, Pearson correlation.  
- **Non-parametric tests:** make fewer assumptions about distribution. Examples: Mann–Whitney, Wilcoxon, Spearman correlation.  
*(Reference: Politi et al., 2021 )*

### 7. What are the assumptions of parametric tests?
- Normal distribution (or approximately normal).  
- Homogeneity of variances.  
- Independence of observations.  
- Linearity (for regression).  
*(Reference: Altman, 1991; Field, 2017)*

### 8. What are the assumptions of non-parametric tests?
- Fewer assumptions are required.  
- Data can be ordinal or non-normal.  
- Independence of observations is still required.  
*(Reference: Mishra et al., 2019)*

### 9. What is a p-value?
The probability of obtaining results as extreme as those observed, if the null hypothesis were true.  
- Small p-value (< 0.05) → evidence against the null hypothesis.  
- Large p-value → data consistent with the null hypothesis.  
*(Reference: Altman, 1991)*

### 10. What is the risk of error when using a statistical test?
- **Type I error (α):** rejecting the null hypothesis when it is true (false positive).  
- **Type II error (β):** failing to reject the null hypothesis when it is false (false negative).  
*(Reference: Howell, 2013)*

### 11. What is the difference between a paired and an unpaired test?
- **Paired test:** used when data are linked (before/after on same subjects, matched pairs).  
- **Unpaired test:** used when comparing independent groups.  
*(Reference: Field, 2017)*
---

# 2. Effect of traitement 
## the data

```{r prépost}
prepost <- read.csv("data/test_data/PrePost.csv", sep=';', header=TRUE)
summary(prepost)

str(prepost)
```

| Variable | Description         | Unit   | Possible values             | Expected change           |
|:---------|:-------------------:|:------:|:---------------------------:|--------------------------:|
| perf     | Performance Measure | bpm    | Numeric values (50–200)     | increase after rehab      |
| time     | Time of measurement | factor | "Before" / "After"          | no change                 |


## 2.2.2 The analysis

#### Does the treatment have an effect?
To determine whether the treatment has an effect, we test the null hypothesis (H₀) that there is no difference in the measured variable(s) between treatment and control (or before and after treatment).  
If the statistical test rejects H₀ (p < 0.05), we conclude that the treatment has a significant effect.  
If H₀ is not rejected (p > 0.05), we conclude that the treatment has no measurable effect on the variables of interest.

### What comparison are you going to make?
The comparison depends on the experimental design:  
- If the same participants were measured before and after the treatment, the appropriate comparison is within-subject (paired), for example comparing PANU values pre- vs post-treatment.  
- If two independent groups were measured (treatment group vs control group), the appropriate comparison is between groups (unpaired).  
In both cases, the aim is to determine whether the treatment condition leads to significantly different values of the variables (PANU, SANU, or EENU).

#### Which test will you use? Parametric or non-parametric?
If the data are normally distributed and variances are homogeneous,  
use a **parametric test** (paired or unpaired t-test, ANOVA).  
If these assumptions are not satisfied,  
use a **non-parametric test** (Wilcoxon signed-rank for paired data, Mann–Whitney for unpaired data).

```{r}
shapiro.test(prepost$Before)
shapiro.test(prepost$After)

prepost$Diff <- prepost$After - prepost$Before
shapiro.test(prepost$Diff)
```
**p<0,05 so it's normal**


#### Which graph illustrates the effect of the treatment?
A **boxplot** to visualize distributions between conditions

```{r}
boxplot(prepost$Before, prepost$After,
        names = c("Before","After"),
        col = c("lightblue","lightgreen"),
        main = "Performance Before vs After",
        ylab = "Performance")
```

#### What sentence will you write in your thesis to explain your result?
*“Our statistical analysis showed a significant effect of the treatment on ... ,  
 p < 0.05, indicating that the treatment condition led to higher values compared to control.”*  
(or, if non-significant: *“… no significant effect was observed, p > 0.05, suggesting that the treatment did not influence ... .”*)

So it's normal, i do a T-test 

```{r}
t.test(prepost$After, prepost$Before, paired = TRUE)
```
**p<0,05 it's significant**

## 2.3 Testing some stereotypes
#### 2.3.1 The data

```{r}
snore <- read.csv("data/test_data/snore.csv", sep=';', header=TRUE)
summary(snore)
```
```{r}
snorers   <- snore$weight[snore$Snorer == "O"]
non_snorers <- snore$weight[snore$Snorer == "N"]

# 3. Test de normalité pour chaque groupe
shapiro.test(snorers)
shapiro.test(non_snorers)

# 4. Choix du test statistique
# Si les 2 groupes ~ Normaux => t-test
t.test(Weight ~ Snorer, data = snore)

# Sinon => test de Mann-Whitney (non-paramétrique)
wilcox.test(Weight ~ Snorer, data = snore)
```


